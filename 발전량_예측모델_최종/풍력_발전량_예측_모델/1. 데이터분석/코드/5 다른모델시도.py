import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import optuna
import warnings

# --- 1. ì´ˆê¸° ì„¤ì • ---
warnings.filterwarnings("ignore")
optuna.logging.set_verbosity(optuna.logging.WARNING) # ë¡œê·¸ë¥¼ ê°„ê²°í•˜ê²Œ

RANDOM_STATE = 42    # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œê°’

# --- 2. ë°ì´í„° ì¤€ë¹„ ---
print("ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
df = pd.read_csv('í’ë ¥_ê³µëª¨ì „/í’ë ¥_ë°œì „ëŸ‰_ì˜ˆì¸¡_ëª¨ë¸/1. ë°ì´í„°ë¶„ì„/ìµœì¢…ì‚¬ìš©_ë°ì´í„°/íŒŒìƒë³€ìˆ˜ì¶”ê°€_ê¸°ìƒê³¼í’ë ¥.csv')

# ì „ì²˜ë¦¬
if df.isnull().sum().sum() > 0: df = df.dropna()
if 'í’ì†(m/s)' in df.columns: df['í’ì†(m/s)_cubed'] = df['í’ì†(m/s)'] ** 3
if 'ë¸”ë ˆì´ë“œ' in df.columns: df['íšŒì „ì²´ë©´ì '] = (df['ë¸”ë ˆì´ë“œ'] ** 2) * np.pi

# íŠ¹ì„±(X) ë° íƒ€ê²Ÿ(y) ì„¤ì •
target = 'ë°œì „ëŸ‰(kWh)'
features = [
    'ì„¤ë¹„ìš©ëŸ‰(MW)', 'ì—°ì‹(ë…„)', 'ê¸°ì˜¨(Â°C)', 'í’ì†(m/s)_cubed', 'ìŠµë„(%)', 'ì¦ê¸°ì••(hPa)', 'ì´ìŠ¬ì ì˜¨ë„(Â°C)',
    'í˜„ì§€ê¸°ì••(hPa)', 'í’í–¥_sin', 'í’í–¥_cos', 'ì‹œê°„_sin', 'ì‹œê°„_cos', 'ì›”_sin', 'ì›”_cos', 'íšŒì „ì²´ë©´ì ',
    'air_density', 'absolute_humidity', 'ê°•ìˆ˜ëŸ‰(mm)'
]
available_features = [col for col in features if col in df.columns]
X = df[available_features]
y = df[target]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)
print(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. í•™ìŠµ: {len(X_train)}ê°œ, ê²€ì¦: {len(X_val)}ê°œ\n")

# --- 3. ëª¨ë¸ë³„ ì‹¤í—˜ í•¨ìˆ˜ ---
def run_experiment(model_name):
    print(f"[{model_name}] ëª¨ë¸ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("  > ë¯¸ë¦¬ ì •ì˜ëœ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ëª¨ë¸ë³„ë¡œ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì •ì˜
    if model_name == 'RandomForest':
        params = {'n_estimators': 854, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 1}
        model = RandomForestRegressor(**params, random_state=RANDOM_STATE, n_jobs=-1)

    elif model_name == 'XGBoost':
        params = {
            'n_estimators': 2872, 'learning_rate': 0.0335762440419045, 'max_depth': 10,
            'subsample': 0.7272212627300844, 'colsample_bytree': 0.9254306650281618
        }
        model = XGBRegressor(**params, tree_method='gpu_hist', predictor='gpu_predictor', random_state=RANDOM_STATE, n_jobs=-1)

    elif model_name == 'LightGBM':
        params = {
            'n_estimators': 2717, 'learning_rate': 0.09759070734303117,
            'num_leaves': 142, 'max_depth': 15
        }
        model = LGBMRegressor(**params, device='gpu', random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)

    elif model_name == 'CatBoost':
        params = {
            'iterations': 3929,
            'learning_rate': 0.08808498552768322,
            'depth': 12,
            'l2_leaf_reg': 1.8083641221733064,
            'bootstrap_type': 'MVS',
            'loss_function': 'MAE',
            'random_seed': 42,
            'verbose': 0,
            'task_type': 'CPU'
        }
        model = CatBoostRegressor(**params)

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)

    # --- (ì°¸ê³ ìš© ì£¼ì„) ì´ì „ì— ì‚¬ìš©í–ˆë˜ ì „ì²´ íŠœë‹ ì½”ë“œ ---
    """
    def objective_template(trial):
        # RandomForest
        rf_params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000), 'max_depth': trial.suggest_int('max_depth', 10, 50),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 32), 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 32),
        }
        # XGBoost
        xgb_params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
        }
        # LightGBM
        lgbm_params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 20, 150), 'max_depth': trial.suggest_int('max_depth', 5, 15),
        }
    
    study = optuna.create_study(direction='minimize')
    study.optimize(objective_template, n_trials=100)
    """
    # --- (ì°¸ê³ ìš© ì£¼ì„) CatBoost íŠœë‹ ì½”ë“œ ---
    # from sklearn.model_selection import GridSearchCV
# param_grid = {
#     'iterations': [400, 600, 800, 1000],
#     'learning_rate': [0.05, 0.07, 0.1],
#     'depth': [6, 8, 10],
#     'l2_leaf_reg': [1, 3, 5],
#     'bootstrap_type': ['Bayesian', 'Bernoulli']
# }
# catboost_model = CatBoostRegressor(loss_function='RMSE', random_seed=42, verbose=0)
# grid_search = GridSearchCV(
#     catboost_model,
#     param_grid,
#     cv=3,
#     scoring='neg_root_mean_squared_error',
#     n_jobs=-1,
#     verbose=2
# )
# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
# print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
# print(f"   ìµœì  CV ì ìˆ˜: {-grid_search.best_score_:.2f}")

    # --- 4. ìµœì¢… í‰ê°€ ---
    y_pred = model.predict(X_val)
    y_pred = np.clip(y_pred, 0, None)  # ìŒìˆ˜ë¥¼ 0ìœ¼ë¡œ í´ë¦¬í•‘

    # ëª¨ë“  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    metrics = {'Model': model_name}
    mae = mean_absolute_error(y_val, y_pred)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    r2 = r2_score(y_val, y_pred)
    y_range = y_val.max() - y_val.min()
    nmae = (mae / y_range) * 100 if y_range > 0 else 0
    nrmse = (rmse / y_range) * 100 if y_range > 0 else 0
    mbe = np.mean(y_pred - y_val)

    eff_mae, eff_rmse, eff_median = np.nan, np.nan, np.nan
    if 'ì„¤ë¹„ìš©ëŸ‰(MW)' in X_val.columns:
        efficiency_true = y_val / X_val['ì„¤ë¹„ìš©ëŸ‰(MW)']
        efficiency_pred = y_pred / X_val['ì„¤ë¹„ìš©ëŸ‰(MW)']  # ì´ë¯¸ í´ë¦¬í•‘ëœ y_pred ì‚¬ìš©
        eff_mae = mean_absolute_error(efficiency_true, efficiency_pred)
        eff_rmse = np.sqrt(mean_squared_error(efficiency_true, efficiency_pred))
        eff_median = np.median(np.abs(efficiency_true - efficiency_pred))

    # ìŒìˆ˜ ì˜ˆì¸¡ ê°œìˆ˜ í™•ì¸ (í´ë¦¬í•‘ ì „ ì›ë³¸ ì˜ˆì¸¡ê°’ì—ì„œ)
    y_pred_original = model.predict(X_val)
    neg_count = np.sum(y_pred_original < 0)
    neg_ratio = (neg_count / len(y_pred_original)) * 100

    print(f"  > ìŒìˆ˜ ì˜ˆì¸¡: {neg_count}ê°œ ({neg_ratio:.2f}%) â†’ 0ìœ¼ë¡œ ì²˜ë¦¬ë¨")

    metrics.update({
        'MAE': mae, 'RMSE': rmse, 'RÂ² Score': r2, 'nMAE (%)': nmae,
        'nRMSE (%)': nrmse, 'MBE': mbe, 'Eff. MAE': eff_mae,
        'Eff. RMSE': eff_rmse, 'Eff. Median Err': eff_median,
        'Negative Predictions': neg_count  # ìŒìˆ˜ ì˜ˆì¸¡ ê°œìˆ˜ë„ ê²°ê³¼ì— í¬í•¨
    })
    return metrics

# --- 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
models_to_run = ['RandomForest', 'XGBoost', 'LightGBM', 'CatBoost']
results_list = []

for model in models_to_run:
    result = run_experiment(model)
    results_list.append(result)
    print("-" * 80)

# ìµœì¢… ê²°ê³¼ ì¢…í•© ë° ì¶œë ¥
results_df = pd.DataFrame(results_list).sort_values(by='RMSE', ascending=True)
pd.set_option('display.float_format', '{:.4f}'.format)
pd.set_option('display.width', 120)

print("\n\nğŸ“Š [ìµœì¢… ê²°ê³¼] ëª¨ë¸ë³„ ìƒì„¸ ì„±ëŠ¥ ë¹„êµ (RMSE ë‚®ì€ ìˆœ ì •ë ¬)")
print(results_df.to_string(index=False))
